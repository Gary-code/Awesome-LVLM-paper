# :sunglasses: Awesome-LVLMs

## Related Collection

* [LLM Hallunication github](https://github.com/HillZhang1999/llm-hallucination-survey)
* [Latest Papers and Datasets on Multimodal Large Language Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)



## Our Paper Reading List

| Folder             | Description                           |
| ------------------ | ------------------------------------- |
| LVLM Model         | Large multimodal models               |
| LVLM Agent         | Agent & Application of LVLM           |
| LVLM Hallucination | Benchmark & Methods for Hallucination |



### :building_construction: LVLM Models

|  Title  |   Venue/Date  |   Note   |   Code   |   Demo   |   Picture   |
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) <br> [**InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning**](https://arxiv.org/pdf/2305.06500.pdf) <br> | NeurIPS 2023 | InstructBLIP | [Github](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) | Local Demo |![instrucblip](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/instructblip_architecture.jpg)|
| ![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star) <br> [**Visual Instruction Tuning**](https://llava-vl.github.io/) <br> | NeurIPS 2023 | LLaVA | [GitHub](https://github.com/haotian-liu/LLaVA) | [Demo](https://llava.hliu.cc/) |![llava](https://llava-vl.github.io/images/llava_arch.png)|
| ![Star](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&label=Star) <br> [**LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model**](https://arxiv.org/pdf/2304.15010.pdf) <br> | 2023-04 | LLaMA Adapter v2 | [Github](https://github.com/OpenGVLab/LLaMA-Adapter) | [Demo](http://llama-adapter.opengvlab.com/) | ![llama](https://github.com/OpenGVLab/LLaMA-Adapter/blob/main/docs/pipeline.png)|
| ![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star) <br> [**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**](https://arxiv.org/pdf/2304.14178.pdf) <br> | 2023-04 | mPLUG | [Github](https://github.com/X-PLUG/mPLUG-Owl) | [Demo](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl) |![m-plug](https://github.com/X-PLUG/mPLUG-Owl/raw/main/assets/model.png)|
| ![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star) <br> [**MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models**](https://arxiv.org/pdf/2304.10592.pdf) <br> | 2023-04 | MiniGPT-4 | [Github](https://github.com/Vision-CAIR/MiniGPT-4) | - |![minigpt-4](https://minigpt-4.github.io/images/overview.png)|
| ![Star](https://img.shields.io/github/stars/SihengLi99/TextBind.svg?style=social&label=Star) <br> [**TextBind: Multi-turn Interleaved Multimodal Instruction-following**](https://arxiv.org/pdf/2309.08637.pdf) <br> | 2023-09 | TextBind | [Github](https://github.com/SihengLi99/TextBind) | [Demo](https://ailabnlp.tencent.com/research_demos/textbind/) |![textbind](![image-20231030213927997](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231030213927997.png)|
| ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) <br> [**InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning**](https://dxli94.github.io/BLIP-Diffusion-website/)<br> | 2023-09 | BLIP-Diffusion|[Github](https://github.com/SihengLi99/TextBind)| [Demo](https://dxli94.github.io/BLIP-Diffusion-website/) |![blip-diffusion](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231030215718639.png)|
| ![Star](https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&label=Star) <br> [**NExT-GPT: Any-to-Any Multimodal LLM**](https://arxiv.org/pdf/2309.05519.pdf) <br> | 2023-09 | NeXT-GPT | [Github](https://github.com/NExT-GPT/NExT-GPT) | [Demo](https://fc7a82a1c76b336b6f.gradio.live/) |![next-gpt](https://next-gpt.github.io/static/images/framework.png)|


### :control_knobs: LVLM Agent

### :face_with_head_bandage: LVLM Hallunication

 
