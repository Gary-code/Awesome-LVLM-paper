# :sunglasses: Awesome-LVLMs

## Related Collection

* [LLM Hallunication github](https://github.com/HillZhang1999/llm-hallucination-survey)
* [Latest Papers and Datasets on Multimodal Large Language Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)



## Our Paper Reading List

| Topic                          | **Description**                                              |
| ------------------------------ | ------------------------------------------------------------ |
| LVLM Model                     | Large multimodal models / Foundation Model                   |
| Multimodal Benchmark & Dataset | :heart_eyes: **Interesting Multimodal Benchmark and Dataset** |
| LVLM Agent                     | Agent & Application of LVLM                                  |
| LVLM Hallucination             | Benchmark & Methods for Hallucination                        |



### :building_construction: LVLM Models

|  Title  |   Venue/Date  |   Note   |   Code   |   Picture   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) <br> [**InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning**](https://arxiv.org/pdf/2305.06500.pdf) <br> | NeurIPS 2023 | InstructBLIP | [Github](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) |![instrucblip](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/instructblip_architecture.jpg)|
| ![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star) <br> [**Visual Instruction Tuning**](https://llava-vl.github.io/) <br> | NeurIPS 2023 | LLaVA | [GitHub](https://github.com/haotian-liu/LLaVA) |![llava](https://llava-vl.github.io/images/llava_arch.png)|
| ![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star) <br> [**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**](https://arxiv.org/pdf/2304.14178.pdf) <br> | 2023-04 | mPLUG | [Github](https://github.com/X-PLUG/mPLUG-Owl) |![image-20241221163809570](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241221163809570.png)|
| ![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star) <br> [**MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models**](https://arxiv.org/pdf/2304.10592.pdf) <br> | 2023-04 | MiniGPT-4 | [Github](https://github.com/Vision-CAIR/MiniGPT-4) |![minigpt-4](https://minigpt-4.github.io/images/overview.png)|
| ![Star](https://img.shields.io/github/stars/SihengLi99/TextBind.svg?style=social&label=Star) <br> [**TextBind: Multi-turn Interleaved Multimodal Instruction-following**](https://arxiv.org/pdf/2309.08637.pdf) <br> | 2023-09 | TextBind | [Github](https://github.com/SihengLi99/TextBind) |![textbind](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231030213927997.png)|
| ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) <br> [**InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning**](https://dxli94.github.io/BLIP-Diffusion-website/)<br> | 2023-09 | BLIP-Diffusion|[Github](https://github.com/SihengLi99/TextBind)|![blip-diffusion](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231030215718639.png)|
| ![Star](https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&label=Star) <br> [**NExT-GPT: Any-to-Any Multimodal LLM**](https://arxiv.org/pdf/2309.05519.pdf) <br> | 2023-09 | NeXT-GPT | [Github](https://github.com/NExT-GPT/NExT-GPT) |![next-gpt](https://next-gpt.github.io/static/images/framework.png)|
| ![Star](https://img.shields.io/github/stars/DCDmllm/Cheetah.svg?style=social&label=Star)<br> [**Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions**](https://arxiv.org/abs/2308.04152) <br> | ICLR 2024 | Multi-image Reasoning | [Github](https://github.com/DCDmllm/Cheetah) |![VPG](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20240126221906220.png)|
| ![Star](https://img.shields.io/github/stars/apple/ml-ferret.svg?style=social&label=Star) <br> [**Ferret: Refer and Ground Anything Anywhere at Any Granularity**](https://arxiv.org/pdf/2310.07704.pdf) <br> | ICLR 2024 | Grounding | [Github](https://github.com/apple/ml-ferret) |![ferret](https://pic1.zhimg.com/70/v2-cb8c4cb634fc5d285ff78b9aa42b270f_1440w.avis?source=172ae18b&biz_tag=Post)|
| ![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star) <br/> [**LLaVA-OneVision: Easy Visual Task Transfer**](https://arxiv.org/abs/2408.03326) <br/> | Technical Report 2024-7 | **LLaVA-OV**: [Blog with details](https://mp.weixin.qq.com/s/sBBICue-kpfYxfkSM_cpwg) | [Project](https://llava-vl.github.io/blog/2024-08-05-llava-onevision/) |![image-20241221110841873](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241221110841873.png)|
| ![Star](https://img.shields.io/github/stars/QwenLM/Qwen2-VL.svg?style=social&label=Star) <br/> [**Qwen2-VL: Enhancing Vision-Language Modelâ€™s Perception of the World at Any Resolution**](https://arxiv.org/pdf/2409.12191) <br/> | Technical Report 2024-10 | **Qwen2-VL**: Dynamic resolution & Multi-images & Video | [Github](https://github.com/QwenLM/Qwen2-VL) |![image-20241221105930185](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241221105930185.png)|
| ![Star](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-VL2.svg?style=social&label=Star) <br/> [**DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding**](https://arxiv.org/pdf/2412.10302) <br/> | Technical Report 2024-12 | **Deepseek-VL2**: MOE<br />Tiny: 1B, Small: 3B DeepSeek-VL2: 5B | [Github](https://github.com/deepseek-ai/DeepSeek-VL2) |![image-20241221110551477](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241221110551477.png)|
| ![Star](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3.svg?style=social&label=Star) <br/> [**DeepSeek-V3 Technical Report**](https://api-docs.deepseek.com/news/news1226) <br/> | Technical Report 2024-12 | ðŸ§  671B MoE parameters<br /> ðŸš€ 37B activated <br />ðŸ“š 14.8T  tokens<br />[Blog](https://www.xiaohongshu.com/explore/676e9f47000000000b00c15d?app_platform=android&ignoreEngage=true&app_version=8.64.0&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBKpSG61_wtNHaO9PF9iQ2S7ttl2eJ2CzVHgczT6SfuSU=&author_share=1&xhsshare=WeixinSession&shareRedId=N0o0NDw5NE82NzUyOTgwNjczOTk3SUg5&apptime=1735356110&share_id=0d9b1634a33c4948b0d25a8d4e2d1a9a&wechatWid=0c7784961e04683226671603d52cb553&wechatOrigin=menu) | [Project](https://github.com/deepseek-ai/DeepSeek-V3) |![image-20241228113108108](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241228113108108.png)|
| ![Star](https://img.shields.io/github/stars/HJYao00/Mulberry.svg?style=social&label=Star) <br/> [**Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search**](https://arxiv.org/pdf/2412.18319) <br/> | 2024-12 | Monte Carlo Tree Search<br />MLLM | [Project](https://github.com/HJYao00/Mulberry) |![image-20250103103936495](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250103103936495.png)|
| ![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star)<br/> [**mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models**](https://arxiv.org/pdf/2408.04840) <br/> | ICLR 2025 | Long Image Sequence | [Project](https://github.com/X-PLUG/mPLUG-Owl) |![image-20250124000043437](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250124000043437.png)|
| ![Star](https://img.shields.io/github/stars/TobiasLee/TempU/.svg?style=social&label=Star)<br/> [**Temporal Reasoning Transfer from Text to Video**](https://video-t3.github.io/) <br /> | ICLR 2025 | Temporal Video | [Project](https://video-t3.github.io/) |![image-20250124000955736](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250124000955736.png)|
| ![Star](https://img.shields.io/github/stars/bytedance/VideoWorld.svg?style=social&label=Star)<br/> [**VideoWorld: Exploring Knowledge Learning from Unlabeled Videos**](https://github.com/bytedance/VideoWorld) <br/> | 2025-01 | Without LLM to Learning the Video | [Project](https://github.com/bytedance/VideoWorld) |![image-20250124000524236](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250124000524236.png)|
| ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA3.svg?style=social&label=Star)<br/> [**VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding**](https://arxiv.org/pdf/2501.13106v1) <br/> | 2025-01 | Video LLaMA Series | [Project](https://github.com/DAMO-NLP-SG/VideoLLaMA3) |![image-20250124002058498](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250124002058498.png)|
| ![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL.svg?style=social&label=Star)<br/>**Qwen2.5-VL Technical Report** | 2025-02 | Qwen2.5-VL | [Project](https://github.com/QwenLM/Qwen2.5-VL) |![image-20250226085649766](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250226085649766.png)|

### :calendar:  Multimodal Benchamrk & Dataset

| Title                                                        |  Venue/Date  |                             Note                             |                             Code                             |                           Picture                            |
| :----------------------------------------------------------- | :----------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![Star](https://img.shields.io/github/stars/MMMU-Benchmark/MMMU.svg?style=social&label=Star) <br/> [**MMMU: A Massive Multi-discipline Multimodal**](https://mmmu-benchmark.github.io) <br/> |  CVPR 2024   |         11K Multimodal Questions Reasoning Benchmark         |         [project](https://mmmu-benchmark.github.io)          | ![algebraic reasoning](https://mmmu-benchmark.github.io/static/images/mmlu_example.Jpeg) |
| ![Star](https://img.shields.io/github/stars/LightChen233/M3CoT.svg?style=social&label=Star) <br/> [**M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought**](https://lightchen233.github.io/m3cot.github.io/) <br/> |   ACL 2024   |    **Multimodal COT**: Multi-step visual modal reasoning     |  [project](https://lightchen233.github.io/m3cot.github.io/)  | ![image-20241221112255186](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241221112255186.png) |
| ![Star](https://img.shields.io/github/stars/Gary-code/PEIFG.svg?style=social&label=Star) <br/> [**Learning to Correction: Explainable Feedback Generation for Visual Commonsense Reasoning Distractor**](https://arxiv.org/abs/2412.07801) <br/> |   MM 2024    |                    Multimodal Correction                     |        [Github](https://github.com/Gary-code/PEIFG/)         | ![image-20241221112534221](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241221112534221.png) |
| ![Star](https://img.shields.io/github/stars/LeoLee7/Directional_guidance.svg?style=social&label=Star) <br/> [**Right this way: Can VLMs Guide Us to See More to Answer Questions?**](https://arxiv.org/abs/2411.00394) <br/> | NeurIPS 2024 |                 For visually impaired people                 |  [Github](https://github.com/LeoLee7/Directional_guidance)   | ![image-20241221163141433](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241221163141433.png) |
| ![Star](https://img.shields.io/github/stars/MM-FIRE/FIRE.svg?style=social&label=Star) <br/> [**FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models**](https://arxiv.org/abs/2407.11522) <br/> | NeurIPS 2024 |               Multimodal Refinement 100K data                |            [Project](https://mm-fire.github.io/)             | ![image-20241226104808366](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241226104808366.png) |
| ![Star](https://img.shields.io/github/stars/zwq2018/Multi-modal-Self-instruct.svg?style=social&label=Star) <br/> [**Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model**](https://arxiv.org/abs/2407.07053) <br/> |  EMNLP 2024  |              Abstract Image Reasoning Benchmark              |    [Project](https://multi-modal-self-instruct.github.io)    | ![image-20241227103310143](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241227103310143.png) |
| ![Star](https://img.shields.io/github/stars/wwzhuang01/Math-PUMA.svg?style=social&label=Star) <br/> [**Math-PUMA: Progressive Upward Multimodal Alignment to Enhance Mathematical Reasoning**](https://arxiv.org/abs/2408.08640) <br/> |  AAAI 2025   |           Math Reasoning &<br />Weak2Strong  Data            |      [Project](https://github.com/wwzhuang01/Math-PUMA)      | ![image-20241226105753500](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241226105753500.png) |
| ![Star](https://img.shields.io/github/stars/eric-ai-lab/MSSBench.svg?style=social&label=Star) <br/> [**Multimodal Situational Safety**](https://arxiv.org/abs/2410.06172) <br/> |  ICLR 2025   |                 Multimodal Safety Benchmark                  |                [Project](mssbench.github.io)                 | ![image-20241223102926454](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241223102926454.png) |
| ![Star](https://img.shields.io/github/stars/eric-ai-lab/MMWorld.svg?style=social&label=Star) <br/> [**MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos**](https://arxiv.org/pdf/2408.13257) <br/> |  ICLR 2025   |                       MMMU in Video QA                       |         [Project](https://mmworld-bench.github.io/)          | ![image-20241223103309015](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241223103309015.png) |
| ![Star](https://img.shields.io/github/stars/yfzhang114/MME-RealWorld.svg?style=social&label=Star)<br/> [**MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?**](https://arxiv.org/pdf/2406.08407) <br/> |  ICLR 2025   |                    High Resolution Image                     |         [Project](https://mme-realworld.github.io/)          | ![image-20250124001834115](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250124001834115.png) |
| ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/multimodal_textbook.svg?style=social&label=Star) <br/> [**2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining**](https://arxiv.org/abs/2501.00958) <br/> |   2025-01    |                Educational Video to Textbook                 | [Project](https://multimodal-interleaved-textbook.github.io/) | ![image-20250108115419174](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250108115419174.png) |
| ![Star](https://img.shields.io/github/stars/VT-NLP/InterleavedBench.svg?style=social&label=Star)<br/> [**Holistic Evaluation for Interleaved Text-and-Image Generation**](https://vt-nlp.github.io/InterleavedEval/) <br/> |  EMNLP 2024  |         Interleaved Text-Image Generation Benchmark          |     [Project](https://vt-nlp.github.io/InterleavedEval/)     | ![image-20250116160734900](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250116160734900.png) |
| ![Star](https://img.shields.io/github/stars/LanceZPF/OpenING.svg?style=social&label=Star)<br/> [**A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**](https://arxiv.org/abs/2406.09403) <br/> |   2024-11    | Interleaved T-I Generation<br />More Scenarios<br />Judge Model |   [Project](https://opening-benchmark.github.io/#examples)   | ![image-20250116161219634](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250116161219634.png) |
| ![Star](https://img.shields.io/github/stars/hychaochao/EMMA.svg?style=social&label=Star)<br/> [**An Enhanced MultiModal ReAsoning Benchmark**](https://emma-benchmark.github.io/) <br/> |   2025-01    |                        Multimodal COT                        |         [Project](https://emma-benchmark.github.io/)         | ![image-20250123235642175](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250123235642175.png) |
| ![Star](https://img.shields.io/github/stars/Lillianwei-h/MMIE.svg?style=social&label=Star)<br/>**MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models** |  ICLR 2025   |          Multimodal COT<br />Interleaved Generation          |           [Project](https://mmie-bench.github.io/)           | ![image-20250218115203269](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250218115203269.png) |
| ![Star](https://img.shields.io/github/stars/USC-GVL/PhysBench.svg?style=social&label=Star)<br/>**PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding** |  ICLR 2025   |                 Physical Wold Understanding                  |           [Project](https://physbench.github.io/)            | ![algebraic reasoning](https://raw.githubusercontent.com/Gary-code/pic/main/img/data_cases_full.png) |
| ![Star](https://img.shields.io/github/stars/Sakshi113/mmau.svg?style=social&label=Star) <br/>**MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark** |  ICLR 2025   |                      MMMU Audio Version                      |    [Project](https://sakshi113.github.io/mmau_homepage/)     | ![image-20250224093821538](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250224093821538.png) |
| ![Star](https://img.shields.io/github/stars/mit-gfx/VLMaterial.svg?style=social&label=Star) <br/>**VLMaterial: Procedural Material Generation with Large Vision-Language Models** |  ICLR 2025   |                   Material -> Python Code                    |       [Project](https://github.com/mit-gfx/VLMaterial)       | ![image-20250224100536382](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20250224100536382.png) |

### :control_knobs: LVLM Agent

|  Title  |   Venue/Date  |   Note   |   Code   |   Picture   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/microsoft/MM-REACT.svg?style=social&label=Star) <br> [**MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action**](https://arxiv.org/pdf/2303.11381.pdf) <br> | 2023-03 | MM-REACT | [Github](https://github.com/microsoft/MM-REACT) |![mm-react](https://multimodal-react.github.io/images/model_figure_2.gif)|
| ![Star](https://img.shields.io/github/stars/allenai/visprog.svg?style=social&label=Star) <br> [**Visual Programming: Compositional visual reasoning without training**](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf) <br> | CVPR 2023 Best Paper | VISPROG (Similar to ViperGPT) | [Github](https://github.com/allenai/visprog) | ![vp](https://prior.allenai.org/assets/project-content/visprog/teaser.png)|
| ![Star](https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&label=Star) <br> [**HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace**](https://arxiv.org/pdf/2303.17580.pdf) <br> | 2023-03 | HuggingfaceGPT | [Github](https://github.com/microsoft/JARVIS) | ![huggingface-gpt](https://easywithai.com/storage/2023/04/HuggingGPT.webp)|
| ![Star](https://img.shields.io/github/stars/lupantech/chameleon-llm.svg?style=social&label=Star) <br> [**Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models**](https://arxiv.org/pdf/2304.09842.pdf) <br> | 2023-04 | Chameleon | [Github](https://github.com/lupantech/chameleon-llm) | ![chameleon](https://chameleon-llm.github.io/images/example.png) |
| ![Star](https://img.shields.io/github/stars/Hxyou/IdealGPT.svg?style=social&label=Star) <br> [**IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models**](https://arxiv.org/pdf/2305.14985.pdf) <br> | 2023-05 | IdealGPT | [Github](https://github.com/Hxyou/IdealGPT) | ![ideal-gpt](https://github.com/Hxyou/IdealGPT/raw/master/figs/main_diagram.jpg) |
| ![Star](https://img.shields.io/github/stars/showlab/assistgpt.svg?style=social&label=Star) <br> [**AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn**](https://arxiv.org/pdf/2306.08640.pdf) <br> | 2023-06 | AssistGPT | [Github](https://github.com/showlab/assistgpt) |![assist-gpt](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231101220152146.png)|
| ![Star](https://img.shields.io/github/stars/thecharm/BDoG.svg?style=social&label=Star)<br/> [**A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning**](https://arxiv.org/pdf/2403.14972) <br/> | ACM MM 2024 | Multi-Agent Debate | [Github](https://github.com/thecharm/BDoG) |![image-20241221111626526](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241221111626526.png)|
| ![Star](https://img.shields.io/github/stars/Yushi-Hu/VisualSketchpad.svg?style=social&label=Star)<br/> [**Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models**](https://arxiv.org/abs/2406.09403) <br/> | NeurIPS 2024 | Draw to facilitate reasoning | [Project](https://visualsketchpad.github.io/) |![image-20241225110818819](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241225110818819.png)|

### :face_with_head_bandage: LVLM Hallunication
|  Title  |   Venue/Date  |   Note   |   Code   |   Picture   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/RUCAIBox/POPE.svg?style=social&label=Star) <br> [**Evaluating Object Hallucination in Large Vision-Language Models**](https://arxiv.org/pdf/2305.10355.pdf) <br> | EMNLP 2023 | Simple Object Hallunicattion Evaluation - POPE | [Github](https://github.com/RUCAIBox/POPE) |![pope](https://github.com/RUCAIBox/POPE/raw/main/assets/POPE.png)|
| ![Star](https://img.shields.io/github/stars/junyangwang0410/HaELM.svg?style=social&label=Star) <br> [**Evaluation and Analysis of Hallucination in Large Vision-Language Models**](https://arxiv.org/abs/2308.15126) <br> | 2023-10 | Hallunicattion Evaluation - HaELM | [Github](https://github.com/junyangwang0410/HaELM) |![HaELM](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231101221948040.png)|
| ![Star](https://img.shields.io/github/stars/FuxiaoLiu/LRV-Instruction.svg?style=social&label=Star) <br> [**Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning**](https://arxiv.org/pdf/2306.14565.pdf) <br> | 2023-06 | GPT4-Assisted Visual Instruction Evaluation (GAVIE) & LRV-Instruction | [Github](https://github.com/FuxiaoLiu/LRV-Instruction) |  ![gavie](https://fuxiaoliu.github.io/LRV/static/images/model.png) |
| ![Star](https://img.shields.io/github/stars/BradyFU/Woodpecker.svg?style=social&label=Star) <br> [**Woodpecker: Hallucination Correction for Multimodal Large Language Models**](https://arxiv.org/pdf/2310.16045.pdf) <br> | 2023-10 | First work to correct hallucinations in LVLMs | [Github](https://github.com/BradyFU/Woodpecker) | ![Woodpecker](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231101222931698.png) |
| ![Star](https://img.shields.io/github/stars/zjunlp/EasyEdit.svg?style=social&label=Star) <br> [**Can We Edit Multimodal Large Language Models?**](https://arxiv.org/pdf/2310.08475.pdf) <br> | EMNLP 2023 | Knowledge Editing Benchmark | [Github](https://github.com/zjunlp/EasyEdit) |![mm-edit](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231106093401334.png)|
| ![Star](https://img.shields.io/github/stars/vl-illusion/dataset.svg?style=social&label=Star) <br> [**Grounding Visual Illusions in Language:Do Vision-Language Models Perceive Illusions Like Humans?**](https://arxiv.org/abs/2311.00047) <br> | EMNLP 2023 | Similar to human illusion? | [Github](https://github.com/vl-illusion/dataset) |![illusion](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231116105241692.png)|
| ![Star](https://img.shields.io/github/stars/vl-rewardbench/VL-RewardBench.svg?style=social&label=Star) <br> [**VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models**](https://arxiv.org/abs/2411.17451) <br> | 2024-11 | Vision-language generative reward | [Project](https://vl-rewardbench.github.io/) | ![image-20241221163651585](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20241221163651585.png) |