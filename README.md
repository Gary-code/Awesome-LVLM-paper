# :sunglasses: Awesome-LVLMs

## Related Collection

* [LLM Hallunication github](https://github.com/HillZhang1999/llm-hallucination-survey)
* [Latest Papers and Datasets on Multimodal Large Language Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)



## Our Paper Reading List

| Folder             | Description                           |
| ------------------ | ------------------------------------- |
| LVLM Model         | Large multimodal models               |
| LVLM Agent         | Agent & Application of LVLM           |
| LVLM Hallucination | Benchmark & Methods for Hallucination |



### :building_construction: LVLM Models

|  Title  |   Venue/Date  |   Note   |   Code   |   Demo   |   Picture   |
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) <br> [**InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning**](https://arxiv.org/pdf/2305.06500.pdf) <br> | NeurIPS 2023 | InstructBLIP | [Github](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) | Local Demo |![instrucblip](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/instructblip_architecture.jpg)|
| ![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star) <br> [**Visual Instruction Tuning**](https://llava-vl.github.io/) <br> | NeurIPS 2023 | LLaVA | [GitHub](https://github.com/haotian-liu/LLaVA) | [Demo](https://llava.hliu.cc/) |![llava](https://llava-vl.github.io/images/llava_arch.png)|
| ![Star](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&label=Star) <br> [**LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model**](https://arxiv.org/pdf/2304.15010.pdf) <br> | 2023-04 | LLaMA Adapter v2 | [Github](https://github.com/OpenGVLab/LLaMA-Adapter) | [Demo](http://llama-adapter.opengvlab.com/) | ![llama](https://github.com/OpenGVLab/LLaMA-Adapter/blob/main/docs/pipeline.png)|
| ![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star) <br> [**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**](https://arxiv.org/pdf/2304.14178.pdf) <br> | 2023-04 | mPLUG | [Github](https://github.com/X-PLUG/mPLUG-Owl) | [Demo](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl) |![m-plug](https://github.com/X-PLUG/mPLUG-Owl/raw/main/assets/model.png)|
| ![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star) <br> [**MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models**](https://arxiv.org/pdf/2304.10592.pdf) <br> | 2023-04 | MiniGPT-4 | [Github](https://github.com/Vision-CAIR/MiniGPT-4) | - |![minigpt-4](https://minigpt-4.github.io/images/overview.png)|
| ![Star](https://img.shields.io/github/stars/SihengLi99/TextBind.svg?style=social&label=Star) <br> [**TextBind: Multi-turn Interleaved Multimodal Instruction-following**](https://arxiv.org/pdf/2309.08637.pdf) <br> | 2023-09 | TextBind | [Github](https://github.com/SihengLi99/TextBind) | [Demo](https://ailabnlp.tencent.com/research_demos/textbind/) |![textbind](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231030213927997.png)|
| ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) <br> [**InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning**](https://dxli94.github.io/BLIP-Diffusion-website/)<br> | 2023-09 | BLIP-Diffusion|[Github](https://github.com/SihengLi99/TextBind)| [Demo](https://dxli94.github.io/BLIP-Diffusion-website/) |![blip-diffusion](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231030215718639.png)|
| ![Star](https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&label=Star) <br> [**NExT-GPT: Any-to-Any Multimodal LLM**](https://arxiv.org/pdf/2309.05519.pdf) <br> | 2023-09 | NeXT-GPT | [Github](https://github.com/NExT-GPT/NExT-GPT) | [Demo](https://fc7a82a1c76b336b6f.gradio.live/) |![next-gpt](https://next-gpt.github.io/static/images/framework.png)|

### :control_knobs: LVLM Agent
|  Title  |   Venue/Date  |   Note   |   Code   |   Demo   |   Picture   |
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/microsoft/MM-REACT.svg?style=social&label=Star) <br> [**MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action**](https://arxiv.org/pdf/2303.11381.pdf) <br> | 2023-03 | MM-REACT | [Github](https://github.com/microsoft/MM-REACT) | [Demo](https://huggingface.co/spaces/microsoft-cognitive-service/mm-react) |![mm-react](https://multimodal-react.github.io/images/model_figure_2.gif)|
| ![Star](https://img.shields.io/github/stars/allenai/visprog.svg?style=social&label=Star) <br> [**Visual Programming: Compositional visual reasoning without training**](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf) <br> | CVPR 2023 Best Paper | VISPROG (Similar to ViperGPT) | [Github](https://github.com/allenai/visprog) | Local Demo | ![vp](https://prior.allenai.org/assets/project-content/visprog/teaser.png)|
| ![Star](https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&label=Star) <br> [**HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace**](https://arxiv.org/pdf/2303.17580.pdf) <br> | 2023-03 | HuggingfaceGPT | [Github](https://github.com/microsoft/JARVIS) | [Demo](https://huggingface.co/spaces/microsoft/HuggingGPT) | ![huggingface-gpt](https://easywithai.com/storage/2023/04/HuggingGPT.webp)|
| ![Star](https://img.shields.io/github/stars/lupantech/chameleon-llm.svg?style=social&label=Star) <br> [**Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models**](https://arxiv.org/pdf/2304.09842.pdf) <br> | 2023-04 | Chameleon | [Github](https://github.com/lupantech/chameleon-llm) | [Demo](https://chameleon-llm.github.io/) | ![chameleon](https://chameleon-llm.github.io/images/example.png) |
| ![Star](https://img.shields.io/github/stars/Hxyou/IdealGPT.svg?style=social&label=Star) <br> [**IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models**](https://arxiv.org/pdf/2305.14985.pdf) <br> | 2023-05 | IdealGPT | [Github](https://github.com/Hxyou/IdealGPT) | Local Demo | ![ideal-gpt](https://github.com/Hxyou/IdealGPT/raw/master/figs/main_diagram.jpg) |
| ![Star](https://img.shields.io/github/stars/showlab/assistgpt.svg?style=social&label=Star) <br> [**AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn**](https://arxiv.org/pdf/2306.08640.pdf) <br> | 2023-06 | AssistGPT | [Github](https://github.com/showlab/assistgpt) | - |![assist-gpt](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231101220152146.png)|

### :face_with_head_bandage: LVLM Hallunication
|  Title  |   Venue/Date  |   Note   |   Code   |   Demo   |   Picture   |
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/RUCAIBox/POPE.svg?style=social&label=Star) <br> [**Evaluating Object Hallucination in Large Vision-Language Models**](https://arxiv.org/pdf/2305.10355.pdf) <br> | EMNLP 2023 | Simple Object Hallunicattion Evaluation - POPE | [Github](https://github.com/RUCAIBox/POPE) | - |![pope](https://github.com/RUCAIBox/POPE/raw/main/assets/POPE.png)|
| ![Star](https://img.shields.io/github/stars/junyangwang0410/HaELM.svg?style=social&label=Star) <br> [**Evaluation and Analysis of Hallucination in Large Vision-Language Models**](https://arxiv.org/abs/2308.15126) <br> | 2023-10 | Hallunicattion Evaluation - HaELM | [Github](https://github.com/junyangwang0410/HaELM) | - |![HaELM](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231101221948040.png)|
| ![Star](https://img.shields.io/github/stars/FuxiaoLiu/LRV-Instruction.svg?style=social&label=Star) <br> [**Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning**](https://arxiv.org/pdf/2306.14565.pdf) <br> | 2023-06 | GPT4-Assisted Visual Instruction Evaluation (GAVIE) & LRV-Instruction | [Github](https://github.com/FuxiaoLiu/LRV-Instruction) | [Demo](https://7b6590ed039a06475d.gradio.live/) |  ![gavie](https://fuxiaoliu.github.io/LRV/static/images/model.png) |
| ![Star](https://img.shields.io/github/stars/BradyFU/Woodpecker.svg?style=social&label=Star) <br> [**Woodpecker: Hallucination Correction for Multimodal Large Language Models**](https://arxiv.org/pdf/2310.16045.pdf) <br> | 2023-10 | First work to correct hallucinations in LVLMs | [Github](https://github.com/BradyFU/Woodpecker) | [Demo](https://60d1b7c6f5408b81d1.gradio.live/) | ![Woodpecker](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231101222931698.png) |